{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Imports and cleans the raw data \"\"\"\n",
    "\"\"\" saves the cleaned data for each month in the data_YYYYMM variable\"\"\"\n",
    "\n",
    "\" Import needed packages\"\n",
    "import pandas as pd\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "\"\"\" Select the data to elaborate and some options \"\"\" \n",
    "\n",
    "\" List the year and month of the data that you want to import as YYYYMM \"\n",
    "dates = [\"202204\", \"202205\",\"202206\",\"202207\",\"202208\",\"202209\"\n",
    "         ,\"202210\",\"202211\",\"202212\",\"202301\",\"202302\",\"202303\"]\n",
    "\n",
    "\" Import data from the web repository or from a local folder \"\n",
    "import_from_local = False # True if you want to import from local.\n",
    "                         # False (default) to import from web repository.\n",
    "folder_path = \"\" # For local import, path to the folder where data \n",
    "                # is stored. Default is the working folder as \"\".\n",
    "    \n",
    "\" Do you want to save the cleaned data locally? \"\n",
    "save_cleaned_data = True # False (default) to not save the data\n",
    "                        # True to save the data locally\n",
    "clean_folder = \"\" # path to the folder where the cleaned data will be\n",
    "                # saved. Default is the working folder as \"\".\n",
    "\n",
    "    \n",
    "\"\"\" The computation part of the script starts from here \"\"\"\n",
    "\n",
    "for date in dates: # iterates over all the selected files\n",
    "    \n",
    "    # Header\n",
    "    print(date + \" cleaning in progress ################################\")\n",
    "    \n",
    "    # Imports the data from web as Pandas DataFrames\n",
    "    if import_from_local is False:\n",
    "        print(\"Importing data from the web\")\n",
    "        url_repository = \"https://divvy-tripdata.s3.amazonaws.com/\"\n",
    "        trail_file_name = \"-divvy-tripdata.zip\"\n",
    "        url = url_repository + date + trail_file_name\n",
    "        response = requests.get(url)\n",
    "        zipfile = ZipFile(BytesIO(response.content))\n",
    "        csvfile = zipfile.open(zipfile.namelist()[0])\n",
    "        data = pd.read_csv(csvfile)\n",
    "    \n",
    "    # Imports the cvs files from the local working folder \n",
    "    # as Pandas DataFrames\n",
    "    if import_from_local is True:\n",
    "        path = folder_path + date + \"-divvy-tripdata.csv\"\n",
    "        data = pd.read_csv(path)\n",
    "    \n",
    "    # Displays some info of the imported data\n",
    "    print(data.head(), \"\\n\") # displays the first rows\n",
    "    data.info()\n",
    "\n",
    "    # Eliminates the empty rows\n",
    "    print(\"\\nEliminating empty rows\")\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Eliminates the duplicate rows\n",
    "    print(\"Eliminating duplicate rows\")\n",
    "    data.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Turns the entries of the columns \"started_at\" and \"ended_at\" \n",
    "    # into datetime format \n",
    "    print(\"\"\"Turning the entries of the columns \"started_at\" \n",
    "    and \"ended_at\" into datetime format\"\"\")\n",
    "    data[\"started_at\"] = pd.to_datetime(data[\"started_at\"])\n",
    "    data[\"ended_at\"] = pd.to_datetime(data[\"ended_at\"])\n",
    "\n",
    "\n",
    "    # Shows all the values of the columns \"rideable_type\", \n",
    "    # \"member_casual\", so I can check that they are valid. \n",
    "    print(\"\"\"values of the columns \"rideable_type\", \"member_casual\" \"\"\")\n",
    "    for i in [\"rideable_type\", \"member_casual\"]: \n",
    "        data_member = data.groupby(i)\n",
    "        entries = data_member.groups.keys()\n",
    "        print(i, \" : \",entries)\n",
    "        print(\" \")\n",
    "\n",
    "    # Shows the max and min of all the data, \n",
    "    # so I can check that the boundaries of the data make sense.\n",
    "    print(\"Shows the MAX and MIN of all the data\")\n",
    "    data_max = pd.DataFrame(\n",
    "                            [data.max(), \n",
    "                             data.min()]\n",
    "                            , index = [\"Max\", \"Min\"]).transpose()\n",
    "    print(data_max)\n",
    "\n",
    "    # Add the column \"ride_length\" \n",
    "    # as subtraction between \"ended_at\" and \"started_at\"\n",
    "    print(\"\\nAdding the column ride_length\")\n",
    "    data[\"ride_length\"] = data[\"ended_at\"] - data[\"started_at\"]\n",
    "\n",
    "    # Add the column \"day_of_week\" that shows the day of the week \n",
    "    # with 0 as Monday and 6 as Sunday \n",
    "    print(\"Adding the column day_of_week\")\n",
    "    data[\"day_of_week\"] = data[\"started_at\"].dt.dayofweek\n",
    "\n",
    "    # Checks if there are negative or zero ride_length values \n",
    "    # and removes these rows\n",
    "    print(\"Removing the rows with negative ride length \")\n",
    "    zero_time = pd.Timedelta(\"0 days 00:00:00\")\n",
    "    right_data = data[data.ride_length > zero_time]\n",
    "    data = right_data\n",
    "\n",
    "    # Shows max and min for the new columns,\n",
    "    # so I can check their boundaries\n",
    "    print(\"Shows max and min for the new columns\")\n",
    "    print(pd.DataFrame(\n",
    "        [data[[\"ride_length\",\"day_of_week\"]].max(),\n",
    "         data[[\"ride_length\",\"day_of_week\"]].min()]\n",
    "        , index=[\"Max\", \"Min\"]).transpose())\n",
    "\n",
    "    # Saves the cleaned data in a csv file\n",
    "    if save_cleaned_data is True:\n",
    "        file_name = clean_folder + \"cleaned_\" + date + \".csv\"\n",
    "        data.to_csv(file_name)\n",
    "        print(f\"\\nSaving the cleaned data in the {file_name} file\")\n",
    "    \n",
    "    # Saves the cleaned data in a global variable for further use \n",
    "    data_name = \"data_\" + date\n",
    "    globals()[data_name] = data\n",
    "    print(f\"\\nSaving the cleaned data in the {data_name} variable\")\n",
    "    \n",
    "    # If running in Jupiter environment, tries to store the data\n",
    "    # for use in a different kernel\n",
    "    try:\n",
    "        if get_ipython():\n",
    "            # Running in a Jupyter environment?\n",
    "            %store globals()[data_name]\n",
    "            print(\"Storing data for use in a different kernel\")\n",
    "    except:\n",
    "        print(\"Not running in a Jupyter environment, may not save global variables\")\n",
    "        pass\n",
    "    \n",
    "    # Shows some info about the cleaned data\n",
    "    print(\"\\nInfos of the cleaned data \")\n",
    "    print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
